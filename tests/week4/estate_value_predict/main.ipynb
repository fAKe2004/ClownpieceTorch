{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfeb887",
   "metadata": {},
   "source": [
    "# Predicting Real Estate Prices with a Neural Network (Week 4)\n",
    "\n",
    "This notebook demonstrates how to build and train a simple neural network for a regression task using the `clownpiece` library. This version incorporates `Dataset`, `Dataloader`, and an `Optimizer`.\n",
    "\n",
    "Our goal is to predict the price of a house based on a set of features. To do this, we will:\n",
    "1.  **Generate a synthetic dataset** and save it to CSV files.\n",
    "2.  **Load the data** using `CSVDataset` and `Dataloader`.\n",
    "3.  **Define a Multi-Layer Perceptron (MLP)** model using `clownpiece.nn.Sequential`.\n",
    "4.  **Train the model** using the `SGD` optimizer and `StepLR` scheduler.\n",
    "5.  **Visualize the training progress** by plotting the loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from clownpiece import Tensor\n",
    "from clownpiece.autograd import no_grad\n",
    "from clownpiece.nn import Module, Linear, Tanh, Sequential, MSELoss, LayerNorm, BatchNorm, LeakyReLU\n",
    "from clownpiece.utils.data.dataset import CSVDataset\n",
    "from clownpiece.utils.data.dataloader import Dataloader\n",
    "from clownpiece.utils.optim.optimizer import SGD\n",
    "from clownpiece.utils.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b464794",
   "metadata": {},
   "source": [
    "### 1. Generating and Saving Synthetic Data\n",
    "\n",
    "First, we generate our synthetic dataset. After generation, we save the training and test sets into `train.csv` and `test.csv` respectively. This simulates a real-world scenario where data is stored in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7866048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "num_train_samples = 2000\n",
    "num_test_samples = 500\n",
    "\n",
    "noise_level = 0  # noise level for price generation\n",
    "\n",
    "# Features: area, house_age, distance_to_city_center, num_convenience_stores, has_yard\n",
    "def get_features(num_samples):\n",
    "  area = np.random.uniform(50, 250, (num_samples, 1))  # area in sq meters\n",
    "  house_age = np.random.uniform(0, 50, (num_samples, 1))  # years\n",
    "  distance_to_city_center = np.random.uniform(1, 25, (num_samples, 1))  # km\n",
    "  num_convenience_stores = np.random.randint(0, 11, (num_samples, 1))\n",
    "  has_yard = np.random.randint(0, 2, (num_samples, 1))\n",
    "  return np.hstack([area, house_age, distance_to_city_center, num_convenience_stores, has_yard])\n",
    "\n",
    "# Price is a non-linear combination of features + noise\n",
    "def get_price(X_raw, noise_level=10):\n",
    "    area, house_age, distance_to_city_center, num_convenience_stores, has_yard = np.hsplit(X_raw, 5)\n",
    "    num_samples = X_raw.shape[0]\n",
    "\n",
    "    # Slightly sublinear area effect (diminishing returns)\n",
    "    area_term = 10 * (area ** 0.95)\n",
    "\n",
    "    # House age penalty: mild quadratic\n",
    "    age_term = -0.5 * (house_age ** 2) + 2.0 * house_age\n",
    "\n",
    "    # Distance: smooth non-linear decay\n",
    "    distance_term = -25 * np.sqrt(distance_to_city_center)\n",
    "\n",
    "    # Convenience stores: log-scale to saturate\n",
    "    store_term = 6 * np.log1p(num_convenience_stores)\n",
    "\n",
    "    # Yard bonus (binary)\n",
    "    yard_term = has_yard * 30.0\n",
    "\n",
    "    # Interactions\n",
    "    area_store_interaction = 0.04 * area * np.log1p(num_convenience_stores)\n",
    "    yard_distance_interaction = -4.0 * has_yard * np.sqrt(distance_to_city_center)\n",
    "    age_distance_interaction = -0.1 * house_age * distance_to_city_center\n",
    "\n",
    "    noise = np.random.randn(num_samples, 1) * noise_level\n",
    "\n",
    "    return (\n",
    "        100 + area_term + age_term + distance_term +\n",
    "        store_term + yard_term +\n",
    "        area_store_interaction + yard_distance_interaction +\n",
    "        age_distance_interaction + noise\n",
    "    )\n",
    "\n",
    "X_train_raw = get_features(num_train_samples)\n",
    "Y_train_raw = get_price(X_train_raw)\n",
    "X_test_raw = get_features(num_test_samples)\n",
    "Y_test_raw = get_price(X_test_raw)\n",
    "\n",
    "# Save to CSV\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "with open('data/train.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(np.hstack([X_train_raw, Y_train_raw]))\n",
    "\n",
    "with open('data/test.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(np.hstack([X_test_raw, Y_test_raw]))\n",
    "\n",
    "print(\"Data saved to data/train.csv and data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014cb2a3",
   "metadata": {},
   "source": [
    "### 2. Loading Data with `CSVDataset` and `Dataloader`\n",
    "\n",
    "Now, we load the data from the CSV files using our custom `CSVDataset`. We also define a transform to process the data on-the-fly. The `Dataloader` will handle batching and shuffling for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization parameters (calculated from the raw training data)\n",
    "X_mean = X_train_raw.mean(axis=0)\n",
    "X_std = X_train_raw.std(axis=0)\n",
    "Y_mean = Y_train_raw.mean(axis=0)\n",
    "Y_std = Y_train_raw.std(axis=0)\n",
    "\n",
    "def data_transform(row):\n",
    "    # Convert string data from CSV to float\n",
    "    row_float = [float(x) for x in row]\n",
    "    \n",
    "    # Separate features and target\n",
    "    features_raw = np.array(row_float[:-1])\n",
    "    target_raw = np.array([row_float[-1]])\n",
    "    \n",
    "    # Normalize\n",
    "    features_norm = (features_raw - X_mean) / X_std\n",
    "    target_norm = (target_raw - Y_mean) / Y_std\n",
    "    \n",
    "    # Convert to Tensors\n",
    "    return Tensor(features_norm.tolist()), Tensor(target_norm.tolist())\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = CSVDataset('data/train.csv', transform=data_transform)\n",
    "test_dataset = CSVDataset('data/test.csv', transform=data_transform)\n",
    "\n",
    "# Create Dataloaders\n",
    "batch_size = 50\n",
    "train_loader = Dataloader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = Dataloader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} training samples and {len(test_dataset)} test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215df0d8",
   "metadata": {},
   "source": [
    "### 3. Defining the Model Architecture\n",
    "\n",
    "We'll use the same MLP architecture as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46791626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_features = 5\n",
    "output_features = 1\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(input_features, 32),\n",
    "    LayerNorm(32),\n",
    "    Tanh(),\n",
    "    Linear(32, 16),\n",
    "    BatchNorm(16),\n",
    "    LeakyReLU(),\n",
    "    Linear(16, output_features)\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb952068",
   "metadata": {},
   "source": [
    "### 4. Training the Model with an Optimizer\n",
    "\n",
    "This time, we use the `SGD` optimizer to handle parameter updates and a `StepLR` to schedule the learning rate. The training loop is now cleaner as it iterates through the `Dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ac81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, optimizer, and scheduler\n",
    "loss_fn = MSELoss()\n",
    "init_lr = 3e-4\n",
    "epochs = 400\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=init_lr)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "# Initialize lists to track losses\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    sum_train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        predictions = model(X_batch)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(predictions, y_batch)\n",
    "        sum_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_train_loss = sum_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "            \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    sum_test_loss = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            test_predictions = model(X_batch)\n",
    "            sum_test_loss += loss_fn(test_predictions, y_batch).item()\n",
    "    avg_test_loss = sum_test_loss / len(test_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 or epoch < 10:\n",
    "        print(f\"Epoch {epoch+1:3}/{epochs}, train loss: {avg_train_loss:.3f}, test loss: {avg_test_loss:.3f}, lr: {optimizer.param_groups[0]['lr']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a3291",
   "metadata": {},
   "source": [
    "### 5. Visualizing the Results\n",
    "\n",
    "Finally, we plot the training and test loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training and Test Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Test Loss', linestyle='--')\n",
    "plt.title(\"Training and Test Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE) Loss\")\n",
    "plt.xlim(0, epochs)\n",
    "plt.ylim(0, max(max(train_losses[5:]), max(test_losses[5:])) * 1.1)\n",
    "print(f\"Min test loss achieved: {min(test_losses):.3f}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
