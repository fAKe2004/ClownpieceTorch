{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebbaf92",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Classification\n",
    "\n",
    "The Fashion-MNIST dataset is a collection of 28Ã—28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. It was designed as a drop-in replacement for the original MNIST dataset (which we used in week 3), serving as a more challenging benchmark for machine learning algorithms while maintaining the same image size, data format, and train/test split (60,000/10,000).\n",
    "\n",
    "Different from week3, instead of using PyTorch's built-in utilities, we'll use our own custom utils from the ClownpieceTorch library.\n",
    "\n",
    "We will:\n",
    "1.  Download the Fashion-MNIST dataset using `torchvision`.\n",
    "2.  Extract the images into a directory structure suitable for `ImageDataset`.\n",
    "3.  Load the data using `ImageDataset` and `Dataloader` and `transforms`.\n",
    "4.  Build and train a simple transformer classifier.\n",
    "5.  Use the `Adam` optimizer and `ExponentialLR` scheduler.\n",
    "6.  Evaluate the model's accuracy and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23975b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "\n",
    "from clownpiece.autograd import no_grad\n",
    "from clownpiece.nn import Module, Linear, ReLU, Sequential, CrossEntropyLoss, MultiheadAttention\n",
    "from clownpiece.utils.data.dataset import ImageDataset, sequential_transform, resize_transform, normalize_transform, to_tensor_transform\n",
    "from clownpiece.utils.data.dataloader import Dataloader\n",
    "from clownpiece.utils.optim.optimizer import Adam\n",
    "from clownpiece.utils.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ae37d",
   "metadata": {},
   "source": [
    "### 1. Download and Extract the Fashion-MNIST Dataset\n",
    "\n",
    "First, we download the Fashion-MNIST dataset using `torchvision`. Then, we define a helper function `extract_to_folders` to save the images into a directory structure that `ImageDataset` can read. The structure will be `data/<split>/<class_name>/<image_index>.png`.\n",
    "\n",
    "This extraction process is only performed once. If the directories already exist, this step is skipped.\n",
    "\n",
    "> **DO NOT** place this notebook in Windows FS (e.g. /mnt/c) while using WSL. The cross-OS file operations are notoriously slow and may consume ~15 minutes to complete the extraction, which only takes ~30s when done locally. This also applies for loading the data at next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec7237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "\n",
    "def extract_to_folders(dataset, root_dir, class_names):\n",
    "    if os.path.exists(root_dir):\n",
    "        print(f\"'{root_dir}' already exists. Skipping extraction.\")\n",
    "        print(f\"(To re-extract, please delete the directory first.)\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(root_dir)\n",
    "    class_names = [\n",
    "        class_name.replace(\"/\", \"_\") for class_name in class_names\n",
    "    ] # avoid T-shirts/top creating neseted folder\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(root_dir, class_name), exist_ok=True)\n",
    "        \n",
    "    for i, (image, label) in tqdm(enumerate(dataset), total=len(dataset), desc=f\"Extracting to {root_dir}\"):\n",
    "        class_name = class_names[label]\n",
    "        image_path = os.path.join(root_dir, class_name, f\"{i}.png\")\n",
    "        image.save(image_path)\n",
    "    print(f\"Extracted {len(dataset)} images to '{root_dir}'.\")\n",
    "\n",
    "def to_numpy(clownpiece_tensor):\n",
    "    \"\"\"Converts a clownpiece.Tensor to a numpy.ndarray.\"\"\"\n",
    "    return np.array(clownpiece_tensor.tolist())\n",
    "\n",
    "# Download the datasets\n",
    "print(\"Downloading Fashion-MNIST...\")\n",
    "train_dataset_raw = datasets.FashionMNIST('./data', train=True, download=True)\n",
    "test_dataset_raw = datasets.FashionMNIST('./data', train=False, download=True)\n",
    "print(\"Download complete.\")\n",
    "\n",
    "class_names = train_dataset_raw.classes\n",
    "\n",
    "# Extract to folder structure\n",
    "extract_to_folders(train_dataset_raw, './data/fashion_mnist/train', class_names)\n",
    "extract_to_folders(test_dataset_raw, './data/fashion_mnist/test', class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bf514",
   "metadata": {},
   "source": [
    "### 2. Define Transformations and Load Data\n",
    "\n",
    "Now we define the image transformations using the functions from our library. We'll resize the images, normalize them, and convert them to `clownpiece` Tensors. Then, we create `ImageDataset` and `Dataloader` instances for both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "img_size = (28, 28)\n",
    "# For grayscale, mean and std are single values\n",
    "mean = 0.2860\n",
    "std = 0.3530 \n",
    "\n",
    "transform = sequential_transform(\n",
    "    resize_transform(img_size),\n",
    "    normalize_transform(mean, std),\n",
    "    to_tensor_transform()\n",
    ")\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = ImageDataset('./data/fashion_mnist/train', transform=transform)\n",
    "test_dataset = ImageDataset('./data/fashion_mnist/test', transform=transform)\n",
    "\n",
    "# Create Dataloaders\n",
    "batch_size = 64\n",
    "train_loader = Dataloader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = Dataloader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} training samples and {len(test_dataset)} test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20b938",
   "metadata": {},
   "source": [
    "### 3. Defining the Model Architecture\n",
    "\n",
    "Our model is a dummy transformer model, which has \n",
    "1. A image embedding layer that partition the image into patches, and apply the same linear kernel.\n",
    "2. A transformer block\n",
    "3. A reduction operator to squeeze out hidden dimension into singleton\n",
    "4. A final linear projection into classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "img_dim = 28\n",
    "input_features = img_dim ** 2 # 28x28 images flattened\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "hidden_dim = 4\n",
    "kernel_size = 4\n",
    "\n",
    "num_patches = (img_dim // kernel_size) ** 2\n",
    "\n",
    "class ImageEmbedding(Module):\n",
    "\n",
    "    def __init__(self, img_dim, hidden_dim, kernel_size = 2):\n",
    "        super().__init__()\n",
    "        self.img_dim = img_dim\n",
    "        self.input_features = img_dim ** 2\n",
    "        self.kernel_size = kernel_size\n",
    "        self.patch_size = kernel_size * kernel_size\n",
    "        \n",
    "        assert img_dim % kernel_size == 0, \"image dimensions must be divisible by kernel_size\"\n",
    "        \n",
    "        self.num_patches = (img_dim // kernel_size) ** 2\n",
    "        \n",
    "        self.kernel = Linear(self.patch_size, hidden_dim)\n",
    "\n",
    "    # (batch_size, input_features) -> (batch_size, num_patches, hidden_dim)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        img_dim = self.img_dim\n",
    "        patch_h, patch_w = self.kernel_size, self.kernel_size\n",
    "        \n",
    "        # Reshape to (batch_size, H, W) to properly extract patches\n",
    "        images = x.reshape([batch_size, img_dim, img_dim])\n",
    "        \n",
    "        # Create patches\n",
    "        # (batch_size, num_patches_h, patch_h, num_patches_w, patch_w)\n",
    "        patches = images.reshape([batch_size, img_dim // patch_h, patch_h, img_dim // patch_w, patch_w])\n",
    "        # Transpose to (batch_size, num_patches_h, num_patches_w, patch_h, patch_w)\n",
    "        patches = patches.transpose(2, 3)\n",
    "        # Reshape to (batch_size, num_patches, patch_size)\n",
    "        patches = patches.reshape([batch_size, self.num_patches, self.patch_size])\n",
    "        \n",
    "        # Project patches to embeddings\n",
    "        return self.kernel(patches)\n",
    "    \n",
    "class TransformerBlock(Module):\n",
    "    # self-attention + mlp\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(hidden_dim, num_heads, True)\n",
    "        \n",
    "        self.mlp = Sequential(\n",
    "            Linear(hidden_dim, ffn_dim),\n",
    "            ReLU(),\n",
    "            Linear(ffn_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    # (batch_size, num_patches, hidden_dim) -> (batch_size, num_patches, hidden_dim)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(x)\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "class ClampAndReduce(Module): \n",
    "    # reduce hidden_dims by mean pooling\n",
    "    def __init__(self, bound = 2.0):\n",
    "        super().__init__()\n",
    "        self.bound = bound\n",
    "        \n",
    "    # (batch_size, num_patches, hidden_dim) -> (batch_size, num_patches)\n",
    "    def forward(self, x): \n",
    "        x = x.clamp(-self.bound, self.bound) \n",
    "        return x.mean(dim=-1)\n",
    "\n",
    "model = Sequential(\n",
    "    ClampAndReduce(), # since it's gray scale image, average pooling to reduce 3 channels\n",
    "    ImageEmbedding(img_dim=img_dim, hidden_dim=hidden_dim, kernel_size=kernel_size),\n",
    "    ReLU(),\n",
    "    TransformerBlock(hidden_dim, num_heads=2, ffn_dim=2*hidden_dim),\n",
    "    ReLU(),\n",
    "    ClampAndReduce(),\n",
    "    Linear(num_patches, num_classes)\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e843c",
   "metadata": {},
   "source": [
    "### 4. Training the Model\n",
    "\n",
    "Here, we set up the training parameters. We use `CrossEntropyLoss` as our loss function, `Adam` optimizer, and an exponential learning rate schedule. We'll track the training loss and test accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fefabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and training parameters\n",
    "loss_fn = CrossEntropyLoss()\n",
    "init_lr = 5e-3\n",
    "epochs = 5\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=init_lr, weight_decay=1e-4)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.6)\n",
    "\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "print_iter_interval = 100\n",
    "earily_stop_interval = 5 # * print_iter_interval\n",
    "\n",
    "def earily_stop(acc_history, earily_stop_interval = earily_stop_interval) -> bool:\n",
    "  # return true if accuracy hasn't improved for last earily_stop_interval acc_history\n",
    "  if len(acc_history) <= earily_stop_interval:\n",
    "    return False\n",
    "  max_acc = max(acc_history[:-earily_stop_interval])\n",
    "  return max_acc >= max(acc_history[-earily_stop_interval:])\n",
    "\n",
    "def test_acc(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with no_grad():\n",
    "        for X_test_batch, y_test_batch in test_loader:\n",
    "            logits_test = model(X_test_batch)\n",
    "            pred = np.argmax(to_numpy(logits_test), axis=1)\n",
    "            correct += np.sum(pred == to_numpy(y_test_batch))\n",
    "            total += y_test_batch.shape[0]\n",
    "    accuracy = 100. * correct / total\n",
    "    model.train()\n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Log learning rate\n",
    "        print(f\">> Epoch {epoch+1:2}/{epochs}, Lr: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        sum_loss = 0.0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(X_batch)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            if math.isnan(loss.item()):\n",
    "                print(f\"<<Training stopped: NaN loss encountered at epoch {epoch}, batch {batch_idx}.\\n\")\n",
    "                return\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if batch_idx % print_iter_interval == 0:\n",
    "                avg_loss = sum_loss / (batch_idx + 1)\n",
    "                train_losses.append(avg_loss)\n",
    "                # Evaluation on test set\n",
    "                accuracy = test_acc(model, test_loader)\n",
    "                test_accuracies.append(accuracy)\n",
    "                \n",
    "                print(f'Train Epoch: {epoch + 1} [{batch_idx * X_batch.shape[0]}/{len(train_loader.dataset)}]\\tLoss: {avg_loss:.6f}, Accuracy: {accuracy:.2f}%', flush=True)\n",
    "                model.train() # Switch back to train mode\n",
    "                \n",
    "                if earily_stop(acc_history=test_accuracies):\n",
    "                    print(f\"<<Training completed: Earily stopped\")\n",
    "                    return\n",
    "        \n",
    "        # Update learning rate at the end of each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    print(\"<<Training completed: Target epochs reached\")\n",
    "    return\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4356d2be",
   "metadata": {},
   "source": [
    "### 5. Visualizing the Results\n",
    "\n",
    "Finally, we plot the training loss and test accuracy over epochs to assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training Loss and Test Accuracy\n",
    "print(f\"Max test accuracy achieved: {max(test_accuracies):.2f}%\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(train_losses)\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.set_xlabel(f\"Iterations (x{print_iter_interval})\")\n",
    "ax1.set_ylabel(\"Cross-Entropy Loss\")\n",
    "ax1.set_ylim(0, max(train_losses) * 1.1)  # Set y-axis limit to 10% above max loss\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(test_accuracies)\n",
    "ax2.set_title(\"Test Accuracy\")\n",
    "ax2.set_xlabel(f\"Iterations (x{print_iter_interval})\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e0f0c",
   "metadata": {},
   "source": [
    "##### If everything works correctly, you will get an accuracy of around 78% at the end of epoch 1, and stop at around 80%.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
