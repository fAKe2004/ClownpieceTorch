{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebbaf92",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Classification with `ImageDataset`\n",
    "\n",
    "This notebook demonstrates how to use the `ImageDataset` and `Dataloader` to train a classifier on the Fashion-MNIST dataset.\n",
    "\n",
    "We will:\n",
    "1.  Download the Fashion-MNIST dataset using `torchvision`.\n",
    "2.  Extract the images into a directory structure suitable for `ImageDataset`.\n",
    "3.  Define image transformations using our library's functions.\n",
    "4.  Load the data using `ImageDataset` and `Dataloader`.\n",
    "5.  Build and train a simple MLP classifier.\n",
    "6.  Use the `Adam` optimizer and `StepLR` scheduler.\n",
    "7.  Evaluate the model's accuracy and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23975b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "\n",
    "from clownpiece import Tensor\n",
    "from clownpiece.autograd import no_grad\n",
    "from clownpiece.nn import Module, Linear, ReLU, Sequential, CrossEntropyLoss\n",
    "from clownpiece.utils.data.dataset import ImageDataset, sequential_transform, resize_transform, normalize_transform, to_tensor_transform\n",
    "from clownpiece.utils.data.dataloader import Dataloader\n",
    "from clownpiece.utils.optim.optimizer import Adam\n",
    "from clownpiece.utils.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ae37d",
   "metadata": {},
   "source": [
    "### 1. Download and Extract the Fashion-MNIST Dataset\n",
    "\n",
    "First, we download the Fashion-MNIST dataset using `torchvision`. Then, we define a helper function `extract_to_folders` to save the images into a directory structure that `ImageDataset` can read. The structure will be `data/<split>/<class_name>/<image_index>.png`.\n",
    "\n",
    "This extraction process is only performed once. If the directories already exist, this step is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ec7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Fashion-MNIST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:06<00:00, 4.29MB/s]\n",
      "\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 148kB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 148kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.19MB/s]\n",
      "\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.49MB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n",
      "Extracted 60000 images to './data/fashion_mnist/train'.\n",
      "Extracted 60000 images to './data/fashion_mnist/train'.\n",
      "Extracted 10000 images to './data/fashion_mnist/test'.\n",
      "Extracted 10000 images to './data/fashion_mnist/test'.\n"
     ]
    }
   ],
   "source": [
    "def extract_to_folders(dataset, root_dir, class_names):\n",
    "    if os.path.exists(root_dir):\n",
    "        print(f\"'{root_dir}' already exists. Skipping extraction.\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(root_dir)\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(root_dir, class_name), exist_ok=True)\n",
    "        \n",
    "    for i, (image, label) in enumerate(dataset):\n",
    "        class_name = class_names[label]\n",
    "        image_path = os.path.join(root_dir, class_name, f\"{i}.png\")\n",
    "        image.save(image_path)\n",
    "    print(f\"Extracted {len(dataset)} images to '{root_dir}'.\")\n",
    "\n",
    "# Download the datasets\n",
    "print(\"Downloading Fashion-MNIST...\")\n",
    "train_dataset_raw = datasets.FashionMNIST('./data', train=True, download=True)\n",
    "test_dataset_raw = datasets.FashionMNIST('./data', train=False, download=True)\n",
    "print(\"Download complete.\")\n",
    "\n",
    "class_names = train_dataset_raw.classes\n",
    "\n",
    "# Extract to folder structure\n",
    "extract_to_folders(train_dataset_raw, './data/fashion_mnist/train', class_names)\n",
    "extract_to_folders(test_dataset_raw, './data/fashion_mnist/test', class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bf514",
   "metadata": {},
   "source": [
    "### 2. Define Transformations and Load Data\n",
    "\n",
    "Now we define the image transformations using the functions from our library. We'll resize the images, normalize them, and convert them to `clownpiece` Tensors. Then, we create `ImageDataset` and `Dataloader` instances for both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcdfef42",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m transform \u001b[38;5;241m=\u001b[39m sequential_transform(\n\u001b[1;32m      8\u001b[0m     resize_transform(img_size),\n\u001b[1;32m      9\u001b[0m     normalize_transform(mean, std),\n\u001b[1;32m     10\u001b[0m     to_tensor_transform()\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create Datasets\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/fashion_mnist/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m ImageDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/fashion_mnist/test\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create Dataloaders\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/22261/Files/Sources/Projects/ClownpieceTorch/tests/week4/fashion_mnist/../../../clownpiece/utils/data/dataset.py:78\u001b[0m, in \u001b[0;36mImageDataset.__init__\u001b[0;34m(self, file_path, transform)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_to_idx \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/22261/Files/Sources/Projects/ClownpieceTorch/tests/week4/fashion_mnist/../../../clownpiece/utils/data/dataset.py:92\u001b[0m, in \u001b[0;36mImageDataset.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n\u001b[1;32m     90\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Convert image to numpy array immediately\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# apply transform\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     transformed_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image_np)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py:921\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    875\u001b[0m ):\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py:274\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_end()\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclusive_fp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_exclusive_fp_after_loading:\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m err_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# still raised if decoder fails to return anything\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "img_size = (28, 28)\n",
    "# For grayscale, mean and std are single values\n",
    "mean = 0.2860 # Calculated from the training set\n",
    "std = 0.3530  # Calculated from the training set\n",
    "\n",
    "transform = sequential_transform(\n",
    "    resize_transform(img_size),\n",
    "    normalize_transform(mean, std),\n",
    "    to_tensor_transform()\n",
    ")\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = ImageDataset('./data/fashion_mnist/train', transform=transform)\n",
    "test_dataset = ImageDataset('./data/fashion_mnist/test', transform=transform)\n",
    "\n",
    "# Create Dataloaders\n",
    "batch_size = 64\n",
    "train_loader = Dataloader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = Dataloader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} training samples and {len(test_dataset)} test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20b938",
   "metadata": {},
   "source": [
    "### 3. Defining the Model Architecture\n",
    "\n",
    "We'll use a simple Multi-Layer Perceptron (MLP) for this classification task. It consists of two hidden layers with ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_features = img_size[0] * img_size[1]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(input_features, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, num_classes)\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e843c",
   "metadata": {},
   "source": [
    "### 4. Training the Model\n",
    "\n",
    "We set up the `CrossEntropyLoss` function, the `Adam` optimizer, and a `StepLR` scheduler. The training loop iterates through the `Dataloader`, performs forward and backward passes, and updates the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15616628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, optimizer, and scheduler\n",
    "loss_fn = CrossEntropyLoss()\n",
    "init_lr = 1e-3\n",
    "epochs = 10\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=init_lr)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Initialize lists to track losses and accuracies\n",
    "train_losses = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fefabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    sum_train_loss = 0\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        # Flatten the image data\n",
    "        X_batch_flat = X_batch.reshape([X_batch.shape[0], -1])\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(X_batch_flat)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "        sum_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_train_loss = sum_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "            \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch_flat = X_batch.reshape([X_batch.shape[0], -1])\n",
    "            logits_test = model(X_batch_flat)\n",
    "            pred = np.argmax(logits_test.tolist(), axis=1)\n",
    "            correct += np.sum(pred == np.array(y_batch.tolist()))\n",
    "            total += y_batch.shape[0]\n",
    "            \n",
    "    accuracy = 100. * correct / total\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:2}/{epochs}, train loss: {avg_train_loss:.4f}, test accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4356d2be",
   "metadata": {},
   "source": [
    "### 5. Visualizing the Results\n",
    "\n",
    "Finally, we plot the training loss and test accuracy over epochs to assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training Loss and Test Accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(train_losses, label='Training Loss')\n",
    "ax1.set_title(\"Training Loss Over Epochs\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Cross-Entropy Loss\")\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(test_accuracies, label='Test Accuracy', color='orange')\n",
    "ax2.set_title(\"Test Accuracy Over Epochs\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
