{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03342eb2",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Classification\n",
    "\n",
    "This notebook demonstrates a classic classification task using the `clownpiece` library to recognize handwritten digits from the MNIST dataset.\n",
    "\n",
    "We will:\n",
    "1.  Load the MNIST dataset using `torchvision`.\n",
    "2.  Preprocess the data and convert it to `clownpiece` Tensors.\n",
    "3.  Build and train a simple MLP classifier.\n",
    "4.  Use `CrossEntropyLoss` for training.\n",
    "5.  Evaluate the model's accuracy on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from clownpiece import Tensor\n",
    "from clownpiece.autograd import no_grad\n",
    "from clownpiece.nn import Module, Linear, ReLU, Sequential, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26fb25",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "To make the code cleaner and more readable, we define two helper functions. `to_CT_tensor` converts a `torch.Tensor` to a `clownpiece.Tensor`, and `to_numpy` converts a `clownpiece.Tensor` back to a `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_CT_tensor(torch_tensor, requires_grad=False):\n",
    "    \"\"\"Converts a torch.Tensor to a clownpiece.Tensor.\"\"\"\n",
    "    return Tensor(torch_tensor.numpy().tolist(), requires_grad=requires_grad)\n",
    "\n",
    "def to_numpy(clownpiece_tensor):\n",
    "    \"\"\"Converts a clownpiece.Tensor to a numpy.ndarray.\"\"\"\n",
    "    return np.array(clownpiece_tensor.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1bfc30",
   "metadata": {},
   "source": [
    "### 1. Loading and Preprocessing the Data\n",
    "\n",
    "We use `torchvision` to load the MNIST dataset. The dataset is transformed into tensors and normalized. We use `torch.utils.data.DataLoader` to create iterators for both the training and test sets, which will feed data to our model in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset using torchvision\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(\"MNIST dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fce87",
   "metadata": {},
   "source": [
    "### 2. Defining the Model Architecture\n",
    "\n",
    "Our model is a simple Multi-Layer Perceptron (MLP) created using the `Sequential` container. It consists of three linear layers with ReLU activation functions in between. The input layer takes flattened 28x28 images (784 features), and the final layer outputs logits for the 10 digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f592b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_features = 784  # 28x28 images flattened\n",
    "num_classes = 10\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(input_features, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, num_classes)\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6bfb37",
   "metadata": {},
   "source": [
    "### 3. Training the Model\n",
    "\n",
    "Here, we set up the training parameters. We use `CrossEntropyLoss` as our loss function, a fixed `learning_rate` of 0.01, and train for 3 epochs. We'll track the training loss and test accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dec436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and training parameters\n",
    "loss_fn = CrossEntropyLoss()\n",
    "learning_rate = 1e-2\n",
    "epochs = 3\n",
    "train_losses = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e403e09b",
   "metadata": {},
   "source": [
    "The training loop iterates through the dataset for a specified number of epochs. In each iteration (batch), it performs the following steps:\n",
    "1.  **Forward Pass**: Computes the model's predictions (logits).\n",
    "2.  **Loss Calculation**: Measures the difference between predictions and actual labels.\n",
    "3.  **Backward Pass**: Computes gradients of the loss with respect to model parameters.\n",
    "4.  **Weight Update**: Adjusts model weights using gradient descent.\n",
    "5.  **Zero Gradients**: Resets gradients for the next iteration.\n",
    "\n",
    "Every 20 batches, we evaluate the model's accuracy on the entire test set and record both the training loss and test accuracy for later visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# May take up to ~10 minutes\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Flatten the data and convert to clownpiece Tensors\n",
    "        data_flat = data.view(data.shape[0], -1)\n",
    "        X = to_CT_tensor(data_flat)\n",
    "        y = to_CT_tensor(target, requires_grad=False)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        with no_grad():\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.copy_(param - param.grad * learning_rate)\n",
    "        \n",
    "        # Zero gradients\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad = None\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Evaluation on test set\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            with no_grad():\n",
    "                for test_data, test_target in test_loader:\n",
    "                    test_data_flat = test_data.view(test_data.shape[0], -1)\n",
    "                    X_test = to_CT_tensor(test_data_flat)\n",
    "                    \n",
    "                    logits_test = model(X_test)\n",
    "                    pred = np.argmax(to_numpy(logits_test), axis=1)\n",
    "                    correct += np.sum(pred == test_target.numpy())\n",
    "            \n",
    "            accuracy = 100. * correct / len(test_loader.dataset)\n",
    "            test_accuracies.append(accuracy)\n",
    "            \n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]\\tLoss: {loss.item():.6f}, Accuracy: {accuracy:.2f}%', flush=True)\n",
    "            model.train() # Switch back to train mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ee6cf",
   "metadata": {},
   "source": [
    "### 4. Visualizing the Results\n",
    "\n",
    "Finally, we plot the training loss and test accuracy over the course of training. The x-axis for both plots represents the number of iterations (in twenties), allowing us to see how both metrics evolved as the model processed more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c62597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(train_losses)\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.set_xlabel(\"Iterations (x20)\")\n",
    "ax1.set_ylabel(\"Cross-Entropy Loss\")\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(test_accuracies)\n",
    "ax2.set_title(\"Test Accuracy\")\n",
    "ax2.set_xlabel(\"Iterations (x20)\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214df6fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exploratory Task\n",
    "\n",
    "\n",
    "Below is another example model which was the original candidate for this task. (which is a dummy transformer model).\n",
    "\n",
    "However, TAs found that it failed to converge :(\n",
    "  \n",
    "If you are interested, **make a copy of this notebook**, then replace the model definition with the following code. (Perhaps make some change to fix it? Not sure if the model structure is just inpractice)\n",
    "\n",
    "> the task is not graded.\n",
    "\n",
    "```python\n",
    "# Define the model\n",
    "input_features = 784  # 28x28 images flattened\n",
    "num_classes = 10\n",
    "\n",
    "hidden_dim = 32\n",
    "kernel_size = 4\n",
    "\n",
    "class ImageEmbedding(Module):\n",
    "    def __init__(self, input_features, hidden_dim, kernel_size = 4):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.patch_size = kernel_size * kernel_size\n",
    "        \n",
    "        assert input_features % self.patch_size == 0, \"input_features must be divisible by patch_size\"\n",
    "        self.num_patches = self.input_features // self.patch_size\n",
    "        \n",
    "        self.linear = Linear(self.patch_size, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_features)\n",
    "        # Reshape to (batch_size, num_patches, patch_size)\n",
    "        patches = x.reshape((-1, self.num_patches, self.patch_size))\n",
    "        \n",
    "        # Project patches to embeddings\n",
    "        return self.linear(patches) # (batch_size, num_patches, patch_size)\n",
    "    \n",
    "class TransformerBlock(Module):\n",
    "    def __init__(self, hidden_dim, num_heads, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(hidden_dim, num_heads, True)\n",
    "        \n",
    "        self.mlp = Sequential(\n",
    "            Linear(hidden_dim, ffn_dim),\n",
    "            ReLU(),\n",
    "            Linear(ffn_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = x + self.mlp(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        return x\n",
    "    \n",
    "class Reduce(Module):\n",
    "    def forward(self, x):\n",
    "        return x.mean(dim=-1)\n",
    "\n",
    "model = Sequential(\n",
    "    ImageEmbedding(input_features=input_features, hidden_dim=hidden_dim, kernel_size=kernel_size),\n",
    "    ReLU(),\n",
    "    TransformerBlock(hidden_dim, num_heads=4, ffn_dim=2*hidden_dim),\n",
    "    ReLU(),\n",
    "    Reduce(),\n",
    "    Linear(input_features // (kernel_size * kernel_size), num_classes)\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
