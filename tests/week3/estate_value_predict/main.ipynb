{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d909a80",
   "metadata": {},
   "source": [
    "# Predicting Real Estate Prices with a Neural Network\n",
    "\n",
    "This notebook demonstrates how to build and train a simple neural network for a regression task using the `clownpiece` library.\n",
    "\n",
    "Our goal is to predict the price of a house based on a set of features. To do this, we will:\n",
    "1.  **Generate a synthetic dataset** that mimics real-world housing data, including features like area, age, and location.\n",
    "2.  **Define a Multi-Layer Perceptron (MLP)** model using `clownpiece.nn.Sequential`.\n",
    "3.  **Train the model** by minimizing the Mean Squared Error (MSE) between its predictions and the true prices.\n",
    "4.  **Visualize the training progress** by plotting the loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from clownpiece import Tensor\n",
    "from clownpiece.autograd import no_grad\n",
    "from clownpiece.nn import Module, Linear, Tanh, Sequential, MSELoss, LayerNorm, BatchNorm, LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c96e0d",
   "metadata": {},
   "source": [
    "### 1. Generating Synthetic Data\n",
    "\n",
    "First, we need data to train our model on. Instead of loading a real dataset, we'll generate a synthetic one. This gives us full control over the features and their relationship with the target variable (price).\n",
    "\n",
    "We will create the following features:\n",
    "- `area`: The square footage of the house.\n",
    "- `house_age`: The age of the house in years.\n",
    "- `distance_to_city_center`: The distance from the house to the city center in kilometers.\n",
    "- `num_convenience_stores`: The number of convenience stores in the vicinity.\n",
    "- `has_yard`: A binary feature indicating whether the house has a yard (1) or not (0).\n",
    "\n",
    "The price will be a non-linear combination of these features with some added random noise to make the task more realistic. Finally, we'll normalize the features to have a mean of 0 and a standard deviation of 1. Normalization is a crucial preprocessing step that helps stabilize and speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "num_train_samples = 2000\n",
    "num_test_samples = 500\n",
    "\n",
    "noise_level = 0  # noise level for price generation\n",
    "\n",
    "# Features: area, house_age, distance_to_city_center, num_convenience_stores, has_yard\n",
    "def get_features(num_samples):\n",
    "  area = np.random.uniform(50, 250, (num_samples, 1))  # area in sq meters\n",
    "  house_age = np.random.uniform(0, 50, (num_samples, 1))  # years\n",
    "  distance_to_city_center = np.random.uniform(1, 25, (num_samples, 1))  # km\n",
    "  num_convenience_stores = np.random.randint(0, 11, (num_samples, 1))\n",
    "  has_yard = np.random.randint(0, 2, (num_samples, 1))\n",
    "  return np.hstack([area, house_age, distance_to_city_center, num_convenience_stores, has_yard])\n",
    "\n",
    "# Price is a non-linear combination of features + noise\n",
    "import numpy as np\n",
    "\n",
    "def get_price(X_raw, noise_level=10):\n",
    "    area, house_age, distance_to_city_center, num_convenience_stores, has_yard = np.hsplit(X_raw, 5)\n",
    "    num_samples = X_raw.shape[0]\n",
    "\n",
    "    # Slightly sublinear area effect (diminishing returns)\n",
    "    area_term = 10 * (area ** 0.95)\n",
    "\n",
    "    # House age penalty: mild quadratic\n",
    "    age_term = -0.5 * (house_age ** 2) + 2.0 * house_age\n",
    "\n",
    "    # Distance: smooth non-linear decay\n",
    "    distance_term = -25 * np.sqrt(distance_to_city_center)\n",
    "\n",
    "    # Convenience stores: log-scale to saturate\n",
    "    store_term = 6 * np.log1p(num_convenience_stores)\n",
    "\n",
    "    # Yard bonus (binary)\n",
    "    yard_term = has_yard * 30.0\n",
    "\n",
    "    # Interactions\n",
    "    area_store_interaction = 0.04 * area * np.log1p(num_convenience_stores)\n",
    "    yard_distance_interaction = -4.0 * has_yard * np.sqrt(distance_to_city_center)\n",
    "    age_distance_interaction = -0.1 * house_age * distance_to_city_center\n",
    "\n",
    "    noise = np.random.randn(num_samples, 1) * noise_level\n",
    "\n",
    "    return (\n",
    "        100 + area_term + age_term + distance_term +\n",
    "        store_term + yard_term +\n",
    "        area_store_interaction + yard_distance_interaction +\n",
    "        age_distance_interaction + noise\n",
    "    )\n",
    "\n",
    "X_raw = get_features(num_train_samples)\n",
    "Y_raw = get_price(X_raw)\n",
    "\n",
    "# Normalize features / label for better training stability\n",
    "X_mean = X_raw.mean(axis=0)\n",
    "X_std = X_raw.std(axis=0)\n",
    "Y_mean = Y_raw.mean(axis=0)\n",
    "Y_std = Y_raw.std(axis=0)\n",
    "\n",
    "def normalize_X(X_raw, X_mean=X_mean, X_std=X_std):\n",
    "    return (X_raw - X_mean) / X_std\n",
    "def normalize_Y(Y_raw, Y_mean=Y_mean, Y_std=Y_std):\n",
    "    return (Y_raw - Y_mean) / Y_std\n",
    "def denormalize_Y(Y_norm, Y_mean=Y_mean, Y_std=Y_std):\n",
    "    return Y_norm * Y_std + Y_mean\n",
    "\n",
    "\n",
    "# Get train set\n",
    "X = Tensor(normalize_X(X_raw).tolist(), requires_grad=False)\n",
    "y = Tensor(normalize_Y(Y_raw).tolist(), requires_grad=False)\n",
    "\n",
    "# Get test set\n",
    "X_test_raw = get_features(num_test_samples)\n",
    "Y_test_raw = get_price(X_test_raw)\n",
    "X_test = Tensor(normalize_X(\n",
    "                X_test_raw).tolist(), \n",
    "                requires_grad=False)\n",
    "y_test = Tensor(normalize_Y(\n",
    "                Y_test_raw).tolist(), requires_grad=False)\n",
    "\n",
    "print(\"Train Features shape:\", X.shape)\n",
    "print(\"Train Target shape:\", y.shape)\n",
    "print(\"Test Features shape:\", X_test.shape)\n",
    "print(\"Test Target shape:\", y_test.shape)\n",
    "\n",
    "print(\"Sample X, y:\")\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497d492",
   "metadata": {},
   "source": [
    "### 2. Defining the Model Architecture\n",
    "\n",
    "Now we'll define our neural network. We'll use a simple Multi-Layer Perceptron (MLP), which is a good starting point for many regression problems.\n",
    "\n",
    "Our model will be constructed using the `Sequential` container, which makes it easy to stack layers in order. The architecture will be:\n",
    "- A `Linear` layer that takes our 5 input features and maps them to 32 hidden units.\n",
    "- A `LayerNorm` layer to normalize the activations.\n",
    "- A `Tanh` activation function to introduce non-linearity.\n",
    "- Another `Linear` layer from 32 to 16 units.\n",
    "- A `BatchNorm` layer.\n",
    "- Another `LeakyReLU` activation.\n",
    "- A final `Linear` layer that maps the 16 hidden units to a single output value: the predicted price.\n",
    "\n",
    "This is quite a dummy model out of nowhere. Let's see if it can fit the price.\n",
    "\n",
    "> P.S.: Applying `LayerNorm, BatchNorm` in such a small model is usually unnecessary (and potentially harmful). We use it only for demonstrative purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_features = 5\n",
    "output_features = 1\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(input_features, 32),\n",
    "    LayerNorm(32),\n",
    "    Tanh(),\n",
    "    Linear(32, 16),\n",
    "    BatchNorm(16),\n",
    "    LeakyReLU(),\n",
    "    Linear(16, output_features)\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db782ebc",
   "metadata": {},
   "source": [
    "### 3. Training the Model\n",
    "\n",
    "With the data and model ready, we can begin training. This involves setting up our loss function and then looping through the data for a set number of epochs.\n",
    "\n",
    "- **Loss Function**: We'll use `MSELoss` (Mean Squared Error), which is the standard loss function for regression tasks. \n",
    "- **Training Loop**: For each epoch, we split `X, y` into batches (a batch $\\to$ an iteration); then we perform the following steps for each iteration:\n",
    "    1.  **Forward Pass**: Pass the input data `X` through the model to get predictions.\n",
    "    2.  **Calculate Loss**: Compute the MSE loss between the predictions and the true targets `y`.\n",
    "    3.  **Backward Pass**: Call `loss.backward()` to compute the gradients of the loss with respect to all model parameters.\n",
    "    4.  **Update Weights**: Manually adjust the model's parameters since we do not have an optimizer yet. We subtract the gradient (scaled by the learning rate `lr`) from each parameter to perform **gradient descent**.\n",
    "    5.  **Zero Gradients**: Manually clear the gradients so they don't accumulate in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace935b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and training parameters\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "batch_size = 50\n",
    "init_lr = 3e-4\n",
    "epochs = 400\n",
    "\n",
    "\n",
    "def get_lr(epoch, epochs = epochs, init_lr = init_lr, final_lr = init_lr / 50):\n",
    "  # exponential lr scheduler\n",
    "  beta = math.log(final_lr / init_lr) / epochs\n",
    "  return init_lr * math.exp(beta * epoch)\n",
    "  \n",
    "# Initialize lists to track losses\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    sum_train_loss = 0\n",
    "    for i in range(0, num_train_samples, batch_size):\n",
    "        # Forward pass\n",
    "        model.train()\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "        predictions = model(X_batch)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(predictions, y_batch)\n",
    "        sum_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights (manual gradient descent)\n",
    "        with no_grad():\n",
    "            lr = get_lr(epoch)\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.copy_(param - param.grad * lr)\n",
    "        \n",
    "        # Zero gradients for the next iteration\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad = None\n",
    "                \n",
    "    avg_train_loss = sum_train_loss / (num_train_samples / batch_size)\n",
    "    train_losses.append(avg_train_loss)\n",
    "            \n",
    "    # Predict on test set\n",
    "    with no_grad():\n",
    "        model.eval()\n",
    "        test_predictions = model(X_test)\n",
    "        avg_test_loss = loss_fn(test_predictions, y_test).item()\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 or epoch < 10:\n",
    "        print(f\"Epoch {epoch+1:3}/{epochs}, train loss: {avg_train_loss:.3f}, test loss: {avg_test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e4bf8",
   "metadata": {},
   "source": [
    "### 4. Visualizing the Results\n",
    "\n",
    "Finally, we'll plot the training and test loss over epochs. This is a simple but effective way to diagnose the training process. If the model is learning correctly, we should see the loss decrease steadily over time and then plateau as it converges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb3a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training and Test Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Test Loss', linestyle='--')\n",
    "plt.title(\"Training and Test Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE) Loss\")\n",
    "plt.xlim(0, epochs)\n",
    "plt.ylim(0, max(max(train_losses[0:]), max(test_losses[0:])) * 1.1)\n",
    "print(f\"Min test loss achieved: {min(test_losses):.3f}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0883a86",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If everything works correctly, you should achieve $\\text{test\\_loss}\\approx 0.01$.\n",
    "Inaccurately speaking, it indicates an average error of $0.1 \\sigma$.\n",
    "\n",
    "> P.S.: It's common that test loss fluctuates after convergence: this may due to overfitting or inproper learning rate scheduling (too large at late epochs). Try to adjust `get_lr()` if that bothers you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153564fb",
   "metadata": {},
   "source": [
    "---\n",
    "# Exploratory Task\n",
    "\n",
    "Try to alter the model structure (e.g. remove normalization layers, increase hidden layer number and size) and adjust learning rate to see if it achieves a better test loss or faster/stabler convergence.\n",
    "\n",
    "> This exploratory task is not graded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
